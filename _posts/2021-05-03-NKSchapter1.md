# A New Kind of Science: first impressions...

After reading the first chaper of *A New Kind of Science* by Stephen Wolfram, I am drawn to a short period of reflection to contemplate the outlandishness of the claims Stephen makes in the first 24 pages of his tome. These claims are centered around a new method of analyzing natural phenomena that attempts to identify a simple set of rules to generate any complex pattern. 

The word complex seems to carry a hidden meaning in these initial pages. The only definition given so far is that complexity is a characteristic of behavior that it is impossible to know before running a computation and viewing the behavior in real time. In other words, you can't just solve a differential equation and plug in a time value to the solution. This raises a few questions for me. Is complexity really a categorical variable as Wolfram suggest, or is there a spectrum of complex behavior? Can some forms of complexity be reproduced with simpler rules that run faster or do all phenomena fall into the two buckets containing the two body problem and the human brain. I would guess that there is some sort of spectrum, however I doubt it is continuous. The classic example is the step change in complexity in the transition from modeling the state dynamics of *n = 2* particles versus *n = 3* paticles. What is so special about *n = 3*? When looking an anwer to this question online, I couldn't find anything satisfying. Eventually I was led to the online version of *A New Kind of Science*... I am facinated by this question and excited to get to it further on in the text. However on the other hand, while there are certainly these step changes in complexity, there also seems to be more than two levels. For example, suppose there is a simple set of rules governing the behavior of matter and energy that combined with the multiplicity of their action generate all natural phenomena that we can ovbserve. Well than this computational rule also generates the output of all other computationsl rules. However, the rules are different and take different ammounts of time to run and run on different media. You could imagine going one level up from this fundamental rule to a rule that governs the chemical properties of atoms and chemical compounds. Perhaps your rule ignores the specific locations of chemical compounds in space and instead focuses on structure of the chemical inputs and outputs. Would giving up detailed knowledge of the position and momentum of every atom in a reaction allow you to jump ahead of the fundamental rule governing all matter and energy and predict the chemical output of the computation before running it? The entire book seems to be premised on the idea that this is possible and practical heuristics in chemistry also support this conclusion. However, running the chemical rule would give you partial knowledge of the output of the more general rule before running it so wouldn't this slightly contradict the original definition of complexity put forth? Kind of, but not really. Is there a difference between these two theoretical systems in terms of complexity? I think wolfram would say there isn't, but exactly why is unclear to me. This is the question that will be in my mind as I press on.

A second question that arises is: how is this related to the recent sucess of computational models like neural nets. Assume that it is possible to take any complex phenomenon, cleave away irrelevant details in the output and generate a simple rule that can predict the complex behavior of the important characteristics of the system without doing a real world system. This has profound implications. Consider neural networks. Traditionally, these nets are described mathematically in terms of vector matrix products composed with various non-linear functions. However, from an input output perspective, neural networks are computational models that run a rule defined by the network archtecture and weights and output very complex behavior like human language. As far as rules go, however, neural nets are not simple. The input values are contorted millions of times in different ways until they are spit out by nets that have billions of parameters. Indeed, improvements in predictions have oven scaled linearly with sheer model size? I see two possibilities. First that in the cases where gigantic nets are needed for prediction the details that we are trying to predict are the details that make the overall system intractable. Second, our methods are two crude to reverse engineer simpler rules for these systems. There are certainly neural nets that do not require complex rules. The MNIST problem requires only a few hundred for very satisfying results. When I say satisfying, I mean that I care about the ouput of the MNIST model becaue I find it useful for achieving my goals as a human. I think understanding how these models are connected to the computational rules that govern Wolfram's experiments could shed light on how neural nets could be improved. My guess is that in reality we will see a mixture of these two answers where we can make better computational models, but run up against theoretical complexity limits in our predictions. However, given an understanding of these limits could serve to chanel investment capital much more efficiently. After seeing acomplishments like the protien folding model, I am hopeful, that the complex world that we humans enough is divorced from reality enough such that we may be able to predict more and more phenomena that effect our lives. 

I am immediatly drawn to the idea of assembly instructions as the simple rules that lead to the complexity of modern computing. However, this is a complicated 
HOW DOES RISK versus CISK debate fit into all of this? Arm has clearly won this argument lending one data point to wolframs ideas.......

My final thought after reading the first chapter of *A New Kind of Science* is that, these ideas are not necesarily new. It has been known for a while that some classes of problems could only be solved by algorithms or programs. I think this book has not yet said anything brand new, despite Wolfram's insistance that very simple rules could generate such complexity is surprising. However, I do think the ideas presented in the first chapter are looked at from a vastly different perspective than they are normally thought of in the sphere of science. I hope this perspective is actually the paradigm shift that Wolfram claims and not colored by the author's bias towards his own ideas. I will judge this question on the quality and results of his experiments.